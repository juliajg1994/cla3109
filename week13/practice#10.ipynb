{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The Embedding layer takes at least two arguments:\n",
    "# the number of possible words in the vocabulary, here 1000 (1 + maximum word index),\n",
    "# and the dimensionality of the embeddings, here 32.\n",
    "embedding_layer = tf.keras.layers.Embedding(1000, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285323c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "imdb = tf.keras.datasets.imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size, start_char=1, oov_char=2, index_from=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f62562",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f7df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# The first indices are reserved\n",
    "# Create dict class using key(words)-value(integers)\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "\n",
    "# Set new keys for us \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# Key: integers, Value: words\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(\"Train_data[0]: {}\\n\".format(train_data[0]))\n",
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4389ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce5c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the lenghts of data to use it as an input of model\n",
    "maxlen = 500\n",
    "\n",
    "# Return the 2D Numpy array of shape (train_data, maxlen) \n",
    "# by transfoming the input data with zeros peddings \n",
    "train_data = tf.keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                           value=word_index[\"<PAD>\"],\n",
    "                                                           padding='post', # 'pre' or 'post'\n",
    "                                                           maxlen=maxlen)\n",
    "\n",
    "test_data = tf.keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                          value=word_index[\"<PAD>\"],\n",
    "                                                          padding='post',\n",
    "                                                          maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b32c6",
   "metadata": {},
   "source": [
    "*pad* : 길이가 500보다 짧은 문장은 pad를 넣어 0으로 표현."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec267eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Len of data: {}\".format(len(train_data[0])))\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6bcc0c",
   "metadata": {},
   "source": [
    "Keras Sequential API를 사용하여 모델을 정의\n",
    "- 첫 번째 레이어는 **Embedding Layer** \n",
    "- 이 레이어는 integer로 인코딩된 vocabulary를 갖고 각 단어 인덱스에 대한 embedding vector를 찾음(integer to float). \n",
    "- embedding vector는 모델이 학습될 때 학습됨.\n",
    "- Vector는 출력 layer에서 차원을 추가하여 `(배치, 시퀀스, 임베딩)`(3D tensor)로 나오게 됨.\n",
    "- 다음으로 **GlobalAveragePooling1D layer**는 시퀀스 차원 `(배치, 기능)`(2D 텐서)을 평균화하여 각 데이터에 대한 고정 길이 출력 벡터를 반\u001f\n",
    "\n",
    "<img src=https://jsideas.net/assets/materials/20180104/GAP_GMP.png width=600>\n",
    "\n",
    "- 이 고정 길이 출력 벡터는 16개의 hidden unit이 있는 **Fully Connected (Dense) Layer**를 통과하게 됨.\n",
    "- Sigmoid 함수를 사용하면 이 값은 0과 1 사이의 부동 소수점으로 리뷰가 긍정적일 확률(또는 신뢰 수준)을 나타냄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b173d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
    "  tf.keras.layers.GlobalAveragePooling1D(),\n",
    "  tf.keras.layers.Dense(16, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, train the model\n",
    "history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=30,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e55726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a24132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[]: get the layer information\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(vocab_size):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea34642",
   "metadata": {},
   "source": [
    "[워드 임베딩 프로젝터](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57fa258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
